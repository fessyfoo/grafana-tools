#!/usr/bin/python -tt
#
import datetime
import json
import logging
import logging.handlers
import optparse
import os
import codecs
import re
import shutil
import sys
import tarfile
import tempfile
import urllib
import urllib2
import errno


def grafana_dashboard_backups( elastic_host, elastic_port, dest_dir, es_index_name, debug, working_dir, make_tarball = True):

  '''Saves all Grafana dashboards stored in Elasticsearch in a tarball
  
     Uses Elasticsearch API to get all saved dashboards into a tarball, the 
     tarball contains one file per dashboard and is available in destination dir
     which is parameterized (defaults to /var/opt/grafana-dashboards-backups).'''
  
  #Set Logging to syslog
  try:
    logger = logging.getLogger(__name__)
    if debug:
      logger.setLevel(logging.DEBUG)
    else:
      logger.setLevel(logging.INFO)
  
    formatter = logging.Formatter('%(pathname)s: %(message)s')
  
    syslog_handler = logging.handlers.SysLogHandler(address = '/dev/log')
    syslog_handler.setFormatter(formatter)
    logger.addHandler(syslog_handler)
  except Exception:
    logging.info('Could not set syslog logging handler')


  if debug:
    urllib2.install_opener(urllib2.build_opener(urllib2.HTTPHandler(debuglevel=1)))


  def search(url, params):
    search_url = '%s?%s' %  (url, urllib.urlencode(params))
    try:
      response = urllib2.urlopen(search_url, timeout=5)
    except urllib2.URLError as e:
      if debug:
        logger.critical('Failed accessing: %s' % search_url)
      logger.critical(e)
      sys.exit(1)

    return json.load(response)

  def scan():
    return search(dashboards_url, {'search_type': 'scan', 'scroll': scroll_time})

  def scroll(scroll_id):
    url = '%s/%s' % (scroll_url, scroll_id)
    return search(url, {'scroll': scroll_time})
  
  def cam_case_convert(name):
    '''strips spaces and replace CamCasing.cam with cam_casing_cam'''
  
    s1 = re.sub('([^._])([A-Z][a-z]+)', r'\1_\2', name.replace(' ',''))
    s1 = s1.replace('.','_')
    return re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower()

  def json_dump(data,filename):
    dump_file = codecs.open(filename, 'w', 'utf-8')
    json.dump(data, dump_file, sort_keys = True, indent = 2, ensure_ascii=False)
    dump_file.close()
 
  # TODO a better way?? http://stackoverflow.com/questions/600268/mkdir-p-functionality-in-python
  def mkdir_p(path):
      try:
          os.makedirs(path)
      except OSError as exc: # Python >2.5
          if exc.errno == errno.EEXIST and os.path.isdir(path):
              pass
          else: raise
 
  # Conveniance vars
  es_type        = 'dashboard'
  es_url         = 'http://%s:%s' % (elastic_host, elastic_port)
  dashboards_url = '%s/%s/%s/_search' % (es_url, es_index_name, es_type)
  scroll_url     = '%s/_search/scroll' % es_url
  scroll_time    = '1m'
  work_tmp_dir   = working_dir or tempfile.mkdtemp()
  utc_datetime   = datetime.datetime.utcnow()
  formatted_time = utc_datetime.strftime("%Y-%m-%d-%H%MZ")

  if working_dir:
      mkdir_p(working_dir)
  
  if debug:
    logger.info('Grabbing grafana dashboards from: %s' % dashboards_url)

  scroll_id = scan()['_scroll_id']

  # Create a tarball with all the dashboards and move to target dir
  
  if make_tarball: 
    tarball_name = '%s.%s.tar.gz' % (es_index_name, formatted_time)
    tarball      = os.path.join(work_tmp_dir, tarball_name)
    tar          = tarfile.open(tarball, 'w:gz')

  if not scroll_id:
    raise Exception('Failed to get scroll id')

  while 1:
    data      = scroll(scroll_id)
    hit_count = len(data['hits']['hits'])
    scroll_id = data['_scroll_id']
    if hit_count == 0:
      break

    try:
      for hit in data['hits']['hits']:
        source               = hit['_source']
        did                  = hit['_id']
        dashboard_definition = json.loads(source['dashboard'])
        dashboard_base_name  = cam_case_convert(did)

        dashboard_file_name  = '%s.json' % dashboard_base_name
        dashboard_file_path  = os.path.join(work_tmp_dir, dashboard_file_name)

        json_dump(dashboard_definition, dashboard_file_path)
        if make_tarball:
          tar.add(dashboard_file_path, '%s/%s' % (es_index_name, dashboard_file_name))

        metadata_file_name = '%s.metadata.json' % dashboard_base_name
        metadata_file_path = os.path.join(work_tmp_dir, metadata_file_name)
        source['dashboard'] = 'stored in alternate file %s' % dashboard_file_name
        json_dump(hit, metadata_file_path)
        if make_tarball:
          tar.add(metadata_file_path, '%s/%s' % (es_index_name, metadata_file_name))

        logger.info('Added %s to the dashboards backup tarball' % did)

    except Exception as e:
      logging.critical(e)
      sys.exit(1)
  
  if make_tarball:
    try:
      tar.close()
      tarball_dest = os.path.join(dest_dir, tarball_name)
      os.rename(tarball,tarball_dest)
      logger.info('New grafana dashboards backup at %s' % tarball_dest)
    except Exception as e:
      logging.critical('Failed to move tarball to %s' % dest_dir)
      logging.critical(e)
      sys.exit(1)
  
  # Clean up
  try:
    if not working_dir:
      shutil.rmtree(work_tmp_dir)
  except Exception as e:
    logging.critical(e)
    sys.exit(1)

def main():
  parser = optparse.OptionParser()
  
  parser.add_option('-D', '--debug',
        action  = 'store_true',
        default = False,
        dest    = 'debug',
        help    = 'Debug output (very noisy)')
  
  parser.add_option('-e', '--grafana-elasticsearch-host',
    dest    = 'elastic_host',
    help    = 'The elastic search host FQDN used by grafana',
    metavar = 'ELASTIC_SEARCH_HOST')
  
  parser.add_option('-p', '--grafana-elasticsearch-port',
    default = '9200',
    dest    = 'elastic_port',
    help    = 'The elastic search port used by grafana',
    metavar = 'ELASTIC_SEARCH_PORT')
  
  parser.add_option('-t', '--dest-dir',
    default = '/var/opt/grafana-dashboards-backups',
    dest    = 'dest_dir',
    help    = 'The destination directory where dashboards will be saved',
    metavar = 'DEST_DIR')

  parser.add_option('-i', '--index-name',
    default = 'grafana-dash',
    dest    = 'es_index_name',
    help    = 'the elasticsearch index that contains the dashboards',
    metavar = 'ELASTICSEARCH_INDEX')

  parser.add_option('--working-dir',
    default = "",
    dest    = 'working_dir',
    help    = 'working directory in which to store tree of json files')

  parser.add_option('--no-tarball',
    action  = 'store_false',
    default = True,
    dest    = 'make_tarball',
    help    = 'do not make a tarball, use with --working-dir to just make a tree of files')
  
  (options, args) = parser.parse_args()
  
  if not options.elastic_host:
    parser.error('An elastic search host is required')
  
  elastic_host = options.elastic_host
  elastic_port = options.elastic_port
  dest_dir = options.dest_dir
  debug = options.debug

  grafana_dashboard_backups(elastic_host, elastic_port, dest_dir, options.es_index_name, debug, options.working_dir, options.make_tarball)

if __name__ == '__main__':
  main()
